{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn-DVnejj5fC"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgFvOmc7j5fE"
      },
      "source": [
        "## CID: 01843211\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3lE_wY8j5fE"
      },
      "source": [
        "## Part 1: Quickfire questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4O2FyPej5fF"
      },
      "source": [
        "#### Question 1 (True risk / Empirical risk):\n",
        "\n",
        "Let $(\\mathbf{x},\\mathbf{y})$ be sampled from the data-generating distribution $D$. Let $f$ be a classifier in function space $\\hat{\\mathcal{F}}$ and let $L$ be a loss function which is a metric or pseudo-metric. Then the risk $R$ corresponding to $f$ is defined $$R(f)=\\mathbb{E}_D[L(f(\\mathbf{x}, \\mathbf{y})]$$\n",
        "\n",
        "This definition assumes that $(\\mathbf{x}, \\mathbf{y})$ are drawn from the true underlying distribution $\\mathcal{D}$, making $R(f)$ a measure of how well $f$ is expected to perform on the entire data space, not just the observed samples. Herein lies the key difference between this true risk $R(f)$ and the empirical risk\n",
        "\n",
        "In statistical learning we seek an $\\hat{f}\\in \\hat{\\mathcal{F}}$ which minimises this true risk, i.e\n",
        "$$\\hat{f}\\in\\text{arg min}_{f\\in\\hat{\\mathcal{F}}}R(f)$$\n",
        "\n",
        "However, since $\\mathcal{D}$ is unknown in real-world scenarios, we cannot compute $R(f)$ directly. Instead, we approximate it using the empirical risk $\\hat{R}(f)$, which is calculated based on the observed dataset $D$:\n",
        "\n",
        "\n",
        "$$\\hat{R}(f)=\\frac{1}{n}\\sum_{i=1}^n L(f(\\mathbf{x}^i), \\mathbb{y}^i)$$\n",
        "\n",
        "For large datasets, this is approximately equal to the true risk and allows us to learn a function\n",
        "\n",
        "$$f^*=\\text{arg min}_{f\\in\\mathcal{F}}\\hat{R}(f)$$\n",
        "\n",
        "where $\\mathcal{F}$ is a choice of function class that we believe is likely to contain the true function, or a suitable approximation thereof. If $\\mathcal{F}$ is well-chosen this will approximate the $\\hat{f}$ defined above.\n",
        "\n",
        "The key difference between these two concepts is that the true risk is an unknown, yet crucial, measurement of the suitability of the learned function $f$ on the data distribution $D$. Given the limitations of our finite set of samples $(\\mathbf{x},\\mathbf{y})\\sim D$, we must use a suitable approximation, which is given by the empirical risk. It can be shown that this is the best estimator of the true risk given the samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-YC7Icj5fF"
      },
      "source": [
        "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
        "\n",
        "A rich hypothesis class $\\mathcal{F}$ will result in the learned function minimising the training error. For example, consider a dataset of $n$ points - if the function class is all degree $n-1$ polynomials, then the learned function will perfectly interpolate all points in the dataset.\n",
        "\n",
        "However, this can be undesirable, as often variations in the data are due to random noise. We instead seek a simpler function will will *generalise* better, and thus minimise the *generalisation error*, which is exactly the difference between $f^*$ and $\\hat{f}$ above.\n",
        "\n",
        "Linear regression is an example of a basic function class. We approximate relationships with linear maps from the feature space to the output space. This performs well with simple relationships, and exhibits low variance, however can be biased with more complex, e.g. polynomial, relationships.\n",
        "\n",
        "On th eother hand, neural nets are examples of a very rich function classse can capture much more complex relationships, however will overfit highly, with high variance, for a simple linear map, and therefore will exhibit high generalisation error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFg8Wr2Qj5fG"
      },
      "source": [
        "#### Question 3 (Dataset splitting):\n",
        "\n",
        "In the case that validation data is drawn from the same distribution as training data, it would be fair to assume that model performance on unseen data is similar to that on that validation data.\n",
        "\n",
        "However, there are exceptions to this rule. Consider a time series generated by a non-stationary model, for example S&P 500 close. If the training data is 2023 data, and the validation data is Jan/Feb 2024 data, it would be unreasonable to asssume that accuracy will perform as well when evluated on the remainder of 2024 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szrYhmDCj5fG"
      },
      "source": [
        "#### Question 4 (Occamâ€™s razor):\n",
        "\n",
        "---\n",
        "\n",
        "Occam's Razor suggests that among competing hypotheses that predict equally well, the simplest one should be selected. This simplicity is often equated with a model or explanation that makes the fewest assumptions.\n",
        "\n",
        "In machine learning, Occam's Razor is interpreted as a guideline for model selection. The principle advises choosing the simplest model that adequately fits the data. This approach is grounded in the idea that simpler models are less likely to overfit the training data. Overfitting occurs when a model captures noise or random fluctuations in the training set rather than the underlying distribution, leading to poor generalisation to new, unseen data.\n",
        "\n",
        "When dealing with naturally occurring data, such as images, the application of Occam's Razor becomes particularly relevant. Images are high-dimensional data with complex, intricate structures. Models trained on image data, such as convolutional neural networks (CNNs), can easily become overly complex, with millions of parameters capable of fitting the training data very closely.\n",
        "\n",
        "Applying Occam's Razor in this context means preferring simpler models that still capture the essential patterns in the image data without memorizing specific details. This simplicity helps in generalizing better to unseen images by focusing on broader, more universal features rather than specifics of the training set.\n",
        "\n",
        "For image data, a simpler model according to Occam's Razor would still need to be complex enough to handle the data's inherent complexity but not so complex that it learns the noise or irrelevant details. This balance helps in achieving good performance on new images that were not part of the training process.\n",
        "\n",
        "Simpler models are often more interpretable and computationally efficient. This efficiency is crucial for deploying models in real-world applications where computational resources may be limited, and understanding model predictions is important for trust and transparency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALvZEzZKj5fH"
      },
      "source": [
        "#### Question 5 (Generalisation error):\n",
        "\n",
        "\n",
        "The generalisation error of a model quantifies how well the model performs on new, unseen data compared to the training data on which it was trained. For a \"good\" model, the generalisation error should be small. A small generalisation error indicates that the model effectively captures the underlying patterns or distributions of the data without being overly fitted to the noise or specific details of the training set.\n",
        "\n",
        "A model with a small generalisation error has successfully learned the true underlying patterns in the data. This ability suggests that the model can apply what it has learned from the training data to unseen data, making accurate predictions across a variety of scenarios that were not specifically presented during training.\n",
        "\n",
        "A good model strikes an optimal balance between bias (the error from erroneous assumptions in the learning algorithm) and variance (the error from sensitivity to small fluctuations in the training set). A small generalisation error indicates that the model has achieved this balance, being neither too simple (high bias and unable to capture complex patterns) nor too complex (high variance and overfitting to the training data).\n",
        "\n",
        "Overfitting occurs when a model learns the noise or random fluctuations in the training data rather than the actual signal. A model with a small generalisation error is robust against overfitting, meaning it has generalized well from the training data to unseen data, focusing on the signal rather than the noise.\n",
        "\n",
        "The ultimate goal of a machine learning model is to perform well on real-world data, which is often different in various ways from the data used during training. A small generalisation error implies that the model is likely to be effective and reliable when deployed in real-world applications, accurately handling new examples that reflect the complexities and variations of real-world scenarios.\n",
        "\n",
        "A good model, by definition, is one that generalizes well from the training data to unseen data, evidenced by a small generalisation error. Achieving a small generalization error requires careful model design, including selecting the right model complexity, employing proper training techniques, and using strategies such as cross-validation and regularization to prevent overfitting. Ultimately, the small generalization error of a good model reflects its ability to make accurate predictions across a wide range of data, embodying the core goal of machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKUYTo47j5fH"
      },
      "source": [
        "#### Question 6 (Rademacher complexity pt1):\n",
        "\n",
        "\n",
        "The empirical Rademacher complexity of a function class $ \\mathcal{F} $ with respect to a sample $ S = \\{x_1, x_2, \\ldots, x_n\\} $ is defined as follows:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathcal{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\sigma}\\left[\\sup_{f \\in \\mathcal{F}}\\left(\\frac{2}{n} \\sum_{i=1}^n \\sigma_i f(x_i)\\right)\\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $S$ represents a sample of $n$ points drawn from a distribution $\\mathcal{D}$.\n",
        "- $\\mathcal{F}$ is a class of functions where each function $f$ maps an input space $\\mathcal{X}$ to real numbers $\\mathbb{R}$.\n",
        "- $\\sigma = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)$ is a sequence of independent Rademacher variables, each taking values $-1$ or $+1$ with equal probability.\n",
        "- $\\mathbb{E}_{\\sigma}$ denotes the expectation over all possible realizations of the Rademacher sequence $\\sigma$.\n",
        "\n",
        "The empirical Rademacher complexity measures the expected maximum correlation between the functions in $\\mathcal{F}$ and a random pattern of signs provided by $\\sigma$ on the sample $S$. It serves as an indicator of the function class's ability to fit random noise, which is directly related to its potential for overfitting and its generalization capacity.\n",
        "\n",
        "A high value of empirical Rademacher complexity suggests that the function class $ \\mathcal{F} $ has a strong ability to fit random patterns. Specifically, it means that there exist functions within $ \\mathcal{F} $ that can align closely with random assignments of labels (as represented by the Rademacher variables) to the data points in $ S $. This ability is indicative of $ \\mathcal{F} $'s flexibility or complexity in modeling data. While the capacity to fit data closely might seem desirable, a high empirical Rademacher complexity raises concerns about overfitting. Overfitting occurs when a model captures not just the underlying signal in the data but also the noise. Therefore, a high complexity value warns that models from $ \\mathcal{F} $ might not generalise well to unseen data, as they could be fitting the noise present in the training sample.\n",
        "\n",
        "Mathematically a high empirical Rademacher complexity will result in a looser bound on the generalisation error. Recall:\n",
        "$$\n",
        "\\mathbb{P}_{S \\sim \\mathcal{D}^n}\\left( \\forall f \\in \\mathcal{F},\\, \\left| \\mathbb{E}[L(f, S)] - \\hat{\\mathbb{E}}[L(f, S)] \\right| \\leq 2\\hat{\\mathcal{R}}_S(\\mathcal{F}) + 3\\sqrt{\\frac{\\log(\\frac{2}{\\delta})}{2n}} \\right) \\geq 1 - \\delta\n",
        "$$\n",
        "\n",
        "\n",
        "This bound implies that with high probability (at least $1 - \\delta$), the absolute difference between the true expected loss and the empirical loss for all functions in the class $\\mathcal{F}$ is bounded by the term involving the empirical Rademacher complexity and a term that decreases as the sample size $n$ increases. The presence of $\\hat{\\mathcal{R}}_S(\\mathcal{F})$ in the bound highlights the role of the function class's complexity in determining generalisation performance. A higher empirical Rademacher complexity leads to a looser bound, meaning that models with higher complexity may have a larger difference between their training and test performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEJcNzq8j5fH"
      },
      "source": [
        "#### Question 7 (Rademacher complexity pt2):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txVv-Mxmj5fH"
      },
      "source": [
        "#### Question 8 (Regularisation term in the loss function):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyxqomPUj5fI"
      },
      "source": [
        "#### Question 9 (Momentum gradient descent):\n",
        "\n",
        "In regular Gradient Descent (GD), we could have difficulty choosing the learning rate where the conditioning number $\\frac{\\lambda_n}{\\lambda_0}$\n",
        " is too big. The learning rate, \"descent rate\", would vary widely in different directions: in one direction we might have a big slope and overshoot the optimum, and in another a small slope and we won't reach the optimum!\n",
        "\n",
        "Momentum GD solves this problem by accumulating previous gradient data in the step iteration, i.e. when choosing a new step, previous gradient values are accounted for. This solves the previous problem: if we have slow convergence in one direction the gradiant accumalation would make it faster, if we overshoot on a different direction, the gradients would change sign and the descent would slow down (in that direction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fPpx0FRj5fI"
      },
      "source": [
        "#### Question 10 (Adam):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hswprC7j5fI"
      },
      "source": [
        "#### Question 11 (AdaGrad):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnN7YwABj5fI"
      },
      "source": [
        "#### Question 12 (Decaying Learning Rate):\n",
        "\n",
        "Decaying learning rate can be useful when seeking the minimum in the loss landscape. In the lanscape (i.e. loss values over\n",
        "-dimensional parameter space), we might have many dips and valleys of diffrent sizes, shapes, and slopes. Our trajectory is seeking the minimum loss value, i.e. descending as much as possible. A big learning rate would be helpful in descending big and wide valleys, as it is making \"big leaps\" in the landscape. But as the trajectory descends, the valleys diameter would grow smaller (we increase the resolution, or zooming in). So a big learning rate would make the trajectory jump over and miss the valley instead of going into it. Decaying the learning rate would make the trajectory take small steps towards the valley, and therefore entering it.\n",
        "\n",
        "We can picture it as throwing a marble to a hole in the ground. If the hole is big and far away, we should make a long throw. If the hole is small and near, we should take a short throw. If there is a small hole inside a big valley, we should first take a long throw into the valley, then a short one into the small hole."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taocpmjOj5fI"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 2: Short-ish proofs [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HhqQk0cj5fJ"
      },
      "source": [
        "\n",
        "### Question 2.1: Bounds on the risk [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGGzpXPqj5fJ"
      },
      "source": [
        "#### (1)\n",
        "Let us reframe our problem in the notation of Hoeffding's Inequality (Theorm 4.2 notes) and then substitute in the inequality.\n",
        "\n",
        "For $S$ training sample and $N$ i.i.d variables $\\{\\textbf{x}\\}_{i=1}^N$ distributed according to $D$\n",
        " (these are r.v. and not samples), we assume deterministic targets $y_i = f(x)$ for some $x$. We also assume that our loss function is $L(f(\\textbf{x}), y)=\\mathbb{1}_{f(\\textbf{x})\\ne y}$ .\n",
        "\n",
        "\n",
        "Now, in the notation of Theorm 4.2, consider the random variables $X_i := L(f(\\textbf{x}_i), y_i)$. They are i.i.d because $\\textbf{x}_i$ are i.i.d and they are bounded between $[a_i,b_i]=[0,1] \\forall i \\le N$.\n",
        "\n",
        "Define $S_N := âˆ‘^N_i{X_i}$ and note that $S_N = N\\hat{R}(f)$. Also note, using linearity of expectation and definition of $\\hat{R}(f)$:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathbb{E}[S_N] = \\mathbb{E} [N\\hat{R}(f)] = âˆ‘_i^N \\mathbb{E}[L(f(\\textbf{x}_i), y_i)] = N \\mathbb{E}[L(f(\\textbf{x}_1), y_1)]=NR(f)\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Now, as $X_i$ are bounded i.i.d, we can use Hoeffdings Inequality:\n",
        "\n",
        "Choose $\\tilde{Ïµ}>0$. Then,\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathbb{P}[|S_N - \\mathbb{E}[S_N]| \\ge \\tilde{Ïµ}] \\le  \\exp{(-2\\tilde{Ïµ}^2/âˆ‘_i^N(b_i - a_i)}).\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Using our definitions and relations, this becomes\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathbb{P}[|N\\hat{R}(f) - NR(f)| \\ge \\tilde{Ïµ}] \\le  \\exp{(-2\\tilde{Ïµ}^2/âˆ‘_i^N(1)})=\\exp{\\frac{-2\\tilde{Ïµ}^2}{N}}.\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "And since $\\tilde{Ïµ}$ is arbitrary, we can choose $\\tilde{Ïµ}=NÏµ$ to find:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathbb{P}[|\\hat{R}(f) - R(f)| \\ge Ïµ] \\le  \\exp{(-2Ïµ^2/âˆ‘_i^N(1)})=\\exp{-2N Ïµ^2}.\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Which gives us Corollary 4.6. ðŸ™‚\n",
        "\n",
        "\n",
        "*Remark:* the probability and the expectation are taken with respect to our sample $S$ out of the distribution $D^N$.\n",
        "\n",
        "---\n",
        "\n",
        "#### (2)\n",
        "The interpretation of the result above shows us that the probability of the gen. error to be above or below $Ïµ$ (meaning, the \"tails\" of the $pdf$) is decreasing monotoniclly with the sample size $N$. For a fixed $Ïµ$, if we take the limit of the sample size $N \\rightarrow \\infty$, then the probability drops to zero, meaning that the tails of the gen. error are bigger than $Ïµ$ almost surely.\n",
        "\n",
        "Let us look at the inverse of the result above:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\mathbb{P}[|\\hat{R}(f) - R(f)| < Ïµ] \\le 1 - \\exp{(-2N Ïµ^2)}.\n",
        "\\end{equation}\n",
        "$$\n",
        "Now if we fix $N$ and take $Ïµ â†’ 0$ then we would find that the probability of the gen. error to be zero is zero!\n",
        "\n",
        "In general, this means that there is a \"fight\" between the sample size $N$ and our bound $Ïµ$, only that the bound is quadratic. If we want the gen. error to drop below $Ïµ$ (within a given fixed tolerance), then increasing $N$ quadratically would give us the desired bound!    \n",
        "\n",
        "---\n",
        "#### (3)\n",
        "The bound in Theorm 4.8 tells us that the gen. error is bounded by the log of the size of the hypothesis class. This means that there is a tradeoff:\n",
        "Choosing a big hypothesis class will allow our model to fit complex data better, but it will also raise the bound, so our gen. error could get bigger.\n",
        "However, the log function ensures that the bound goes up slower then our hypothesis class expands, which is helpful.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jghFnAovj5fJ"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.2: On semi-definiteness [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVBKhpkOj5fJ"
      },
      "source": [
        "We will show that a convex function $f: \\mathbb{R}^N \\to \\mathbb{R}$ has a positive semidefinite Hessian, that is, $v^{\\top}âˆ‡^2f(x)v\\geq0\\, âˆ€x, v \\in \\mathbb{R}^n$.\n",
        "\n",
        "\n",
        "Define\n",
        "$$g(t)=f(x+tv)$$ for any arbitrary $x,v \\in \\mathbb{R}^n$. Then by the chain rule we have\n",
        "\\begin{align*}\n",
        "g'&=v^{\\top}\\nabla f(x+tv)\\\\\n",
        "g'' &= v^{\\top}\\nabla^2 f(x+tv)v\n",
        "\\end{align*}\n",
        "\n",
        "which exists as long as $f$ is twice differentiable. Moreover, to see that $g$ is convex, consider the change of variables $y=x+sv, z=x+tv$. Then with some rearranging we see\n",
        "\n",
        "\\begin{align*}\n",
        "  f(y)&\\geq f(z) + \\nabla f(z)(y-z)\\\\\n",
        "  f(x+sv) &\\geq f(x+tv)+\\underbrace{\\nabla f(x+tv)v}_{=g'(t)} (s-t)\\\\\n",
        "  g(s) &\\geq g(t) + g'(t)(s-t)\n",
        "\\end{align*}\n",
        "\n",
        "and therefore $g$ is convex. By the theorem in the notes, we have that $g''(t)>0 \\, âˆ€t\\in\\mathbb{R}$. Therefore,\n",
        "$$g''(t) = v^{\\top}\\nabla^2 f(x+tv)v\\geq0\\,\\,\\forall x,v \\in \\mathbb{R}^n, t \\in \\mathbb{R}$$\n",
        "Letting $t=0$ yields the desired result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyzVC4tQj5fJ"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.3: A quick recap of momentum [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiJgLthrj5fJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOUmPqsHj5fJ"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.4: Convergence proof [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_jjlmzgj5fK"
      },
      "source": [
        "We will prove the following theorem:\n",
        "\n",
        "*Suppose $f\\in C^3(\\mathbb{R})$ and $x^*\\in\\mathbb{R}^d$ such that $\\nabla f(x^*)=0$ and $\\det [\\nabla^2 f(x)]\\neq0$. Then $âˆƒ Ïµ>0$ such that iterations of Newton's method starting from any point $x_0\\in B_{Ïµ} (x^*)$ are well-defined and converge to $x^*$. Moreover the rate of convergence is quadratic.*\n",
        "\n",
        "*Proof.*\n",
        "\n",
        "\n",
        "\n",
        "1.   The evolution of Newton's gradient descent method is governed by the updated rule\n",
        "$$x^{(n+1)}=x^{(n)}-(âˆ‡^2f(x^{(n)}))^{-1}âˆ‡f(x^{(n)})$$\n",
        "for $n=0,1,2,\\dots$. This method will perform a gradient descent on the quadratic approximation of $f$.\n",
        "\n",
        "2.   Let $f(x)=\\frac{1}{2}x^{\\top} Q x + b^{\\top}x+c$. Then the derivative of $f$ is given by\n",
        "$$\\nabla f(x)=\\frac{1}{2}Q^{\\top}x+\\frac{1}{2}Qx+b$$\n",
        "If we further assume that $Q$ is symmetric positive definite, this simplifies to\n",
        "$$\\nabla f(x)=Qx+b$$\n",
        "and moreover\n",
        "$$\\nabla^2 f(x)=Q$$\n",
        "so that $f$ is convex in $\\mathbb{R}$ and hence has a unique minimiser. Let $x^{(0)}\\in\\mathbb{R}^d$ be arbitrary. Then the first update is given by \\begin{align*}x^{(1)}&=x^{(0)}-Q^{-1}(Qx^{(0)}+b)\\\\\n",
        "&=x^{(0)}-x^{(0)}-Q^{-1}b\\\\\n",
        "&=-Q^{-1}b\\end{align*}\n",
        "Consider the gradient at this point: we have $âˆ‡f(x^{(1)})=-QQ^{-1}b+b=0$. By the properties of convex functions this means that $x^{(1)}$ is the unique global minimiser and moreover the Newton update value will clearly equal zero.\n",
        "\n",
        "3. Newton's method converged in a single step for quadratic forms because it optimises a local quadratic approximation to the function. Of course, when the function is quadratic, Newton's method optimises the global function and therefore finds immediately the global minimum by effectively selecting the point at which the gradient is zero.\n",
        "\n",
        "4. We claim that\n",
        "$$\\|x^{(1)}-x^*\\|\\leq\\|(\\nabla^2f(x^{(0)}))^{-1}\\|\\|âˆ‡^2f(x^{(0)})(x^{(0)}-x^*)-\\nabla f(x^{(0)})\\|$$\n",
        "To see this we represent $x^{(1)}$ in the form of Newton's iterative step and multiply out the left hand side. Consider:\n",
        "\\begin{align*}\n",
        "\\|x^{(1)}-x^*\\|&=\\|x^{(0)} - \\nabla^2f(x^{(0)}))^{-1}\\nabla f(x^{(0)}) - x^*\\|\\\\\n",
        "&= \\|\\nabla^2f(x^{(0)}))^{-1}\\left[-\\nabla f(x^{(0)}) - (x^*-x^{(0)})\\nabla^2f(x^{(0)})\\right]\\|\\\\\n",
        "&\\leq \\|\\nabla^2f(x^{(0)}))^{-1}\\|\\|-\\nabla f(x^{(0)}) - (x^*-x^{(0)})\\nabla^2f(x^{(0)})\\|\\\\\n",
        "&= \\|\\nabla^2f(x^{(0)}))^{-1}\\|\\|\\nabla^2f(x^{(0)})(x^{(0)}-x^*)-\\nabla f(x^{(0)})\\|\n",
        "\\end{align*}\n",
        "exactly as required, where we use the triangle-like equality given in Lemma 0.1 to multiply out the norm. We note that the inverse norm $\\|\\nabla^2f(x^{(0)}))^{-1}\\|$ exists and is finite according to Lemma 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkmTiHdaShlu"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.5: Convergence continued\n",
        "\n",
        "We will show that for $x^{(1)}$ defined above in terms of $x^{(0)}$, we have that there exists $\\epsilon>0$ such that for any $x^{(0)}\\in B_{Ïµ}(x^*) \\,âˆƒ c_1, c_2 >0$ satisfying\n",
        "$$\\|x^{(1)}-x^*\\|\\leq c_1 c_2 \\|x^{(0)}-x^*\\|^2$$\n",
        "that is, local convergence to the point $x^*$ is quadratic in the first iteration.\n",
        "\n",
        "Consider \\begin{align*}\n",
        "\\|x^{(1)}-x^*\\|&\\leq\\underbrace{\\|\\nabla^2f(x^{(0)}))^{-1}\\|}_{\\text{(1) bounded by } c_1}\\|\\nabla^2f(x^{(0)})(x^{(0)}-x^*)-\\nabla f(x^{(0)})\\|\\\\\n",
        "&\\leq c_1 \\|\\underbrace{\\nabla^2f(x^{(0)})(x^{(0)}-x^*)-\\nabla f(x^{(0)})}_{(2)}\\|\n",
        "\\end{align*}\n",
        "\n",
        "by Lemma 0.1. We know that $(1)$ is true because by the assumption in the problem statement, $\\nabla^2 f(x^*)$ is invertible. Therefore by Lemma 0.2 there exists an $Ïµ_h>0$ such that $(\\nabla^2 f(x))^{-1}$ exists on $B_{Ïµ_h}(x^*)$. Let $Ïµ_d=\\min\\{Ïµ, Ïµ_h\\}$. Then we may set $c_1=\\sup_{x\\in B_{\\epsilon_d}(x^*)}\\frac{1}{\\|âˆ‡^2f(x)\\|}$, which is well-defined as a result.\n",
        "\n",
        "\n",
        "We now deal with $(2)$. We know that $f$ has a continuous third derivative at $x^*$. By the 2nd order Taylor approximation of $âˆ‡f$ (which is twice continuously differentiable) we have that for some $\\xi$ on the line segment between $x^{(0)}$ and $x^*$:\n",
        "$$0 = \\nabla f(x^*) = âˆ‡ f(x^{(0)}) + \\nabla^2f(x^{(0)})(x^*-x^{(0)}) + \\frac{1}{2}\\nabla^3f(\\xi)(x^*-x^{(0)})^2$$\n",
        "\n",
        "Hence we rewrite $(2)$ as $\\frac{âˆ‡^3f\\left(\\xi\\right)}{2!}(x^*-x^{(0)})^2$ to yield\n",
        "\n",
        "\\begin{align*}\n",
        "\\left\\|\\frac{âˆ‡^3f\\left(\\xi\\right)}{2!}(x-x^{(0)})^2\\right\\|\n",
        "&\\leq \\underbrace{\\left\\|\\frac{âˆ‡^3f\\left(\\xi\\right)}{2}\\right\\|}_{\\leq c_2}\\left\\|(x-x^{(0)})^2\\right\\|\\\\\n",
        "&\\leq c_2 \\left\\|(x-x^{(0)})\\right\\|^2\n",
        "\\end{align*}\n",
        "by Lemma 0.1, where we can bound $\\|\\nabla^3f\\|$ with appropriate selection of sufficiently small $Ïµ$ using the fact that $f\\in C^3(\\mathbb{R})$ and hence $âˆ‡^3f$ is continuous on $B_{\\epsilon}(x^*)$. We then set $c_2=\\frac{1}{2}\\sup_{x\\in B_{Ïµ}(x^*)}\\|âˆ‡^3f(x)\\|$ Putting all this together we obtain the desired result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOAL1Q-mdgOn"
      },
      "source": [
        "***\n",
        "### Question 2.6: Convergence Continued\n",
        "\n",
        "We will now show that if $x^{(0)}\\in B_{\\epsilon}(x^*)$ for some $Ïµ>0$ satisfies\n",
        "$$\\|x^{(0)}-x^*\\|\\leq\\frac{\\alpha}{c_1c_2}$$\n",
        "for some $0<\\alpha<1$ then we have that $x_1\\in B_{\\epsilon}(x^*)$ and moreover that\n",
        "$$\\|x^{(1)}-x^*\\|\\leq\\frac{\\alpha}{c_1c_2}$$\n",
        "\n",
        "First, note that if $x^{(0)}\\in B_{\\epsilon}(x^*)$ and $\\|x^{(0)}-x^*\\|\\leq\\frac{\\alpha}{c_1c_2}$ then we can assume that $\\epsilon\\leq\\frac{\\alpha}{c_1c_2}$.\n",
        "\n",
        "From the previous section we have that \\begin{align*}\\|x^{(1)}-x^*\\|&\\leq c_1 c_2 \\|x^{(0)}-x^*\\|^2\\\\\n",
        "&\\leq c_1 c_2 Ïµ^2\\\\\n",
        "&\\leq Ïµ \\end{align*}\n",
        "for $\\epsilon\\leq \\frac{1}{c_1c_2}$. This is implied by the fact that $\\alpha < 1$. Hence $x_1\\in B_{\\epsilon}(x^*)$. If moreover $Ïµ\\leq\\frac{\\surd{\\alpha}}{c_1c_2}$ then the desired equality holds, noting this is implied by the assumption since $\\alpha<1$ and so $\\frac{\\surd{\\alpha}}{c_1c_2}\\geq\\frac{\\alpha}{c_1c_2}\\geq\\epsilon$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSIbCQxyMy1U"
      },
      "source": [
        "***\n",
        "\n",
        "Question 2.7: Convergence continued\n",
        "\n",
        "Let $x^{(k)}\\in B_{Ïµ}(x^*), k \\in \\mathbb{N}$ arbitrary. We will derive an upper bound for $\\|x^{(k+1)}-x^*\\|$ in terms of $\\|x^{(k)}-x^*\\|$ where $x^{(k)} \\in B_{\\frac{\\alpha}{c_1c_2}}(x^*)$.\n",
        "\n",
        "Using 2.7, given we made no assumption on the nature of $x_0$, the upper bound is exactly the same. Specifically, we have that if\n",
        "$$\\|x^{(k)}-x^*\\|\\leq \\frac{Î±}{c_1c_2}$$\n",
        "then from 2.7\n",
        "$$\\|x^{(k+1)}-x^{*}\\|\\leq \\frac{\\alpha ^2}{c_1c_2}$$\n",
        "\n",
        "noting that $\\alpha^2 \\leq\n",
        "Î±$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "Inductively, for all $k \\in \\mathbb{N}_0$ we have that $\\|x^{(k)}-x^*\\|\\leq\\frac{Î±}{c_1c_2}$ implies $\\|x^{(k+1)}-x^*\\|\\leq\\frac{Î±^2}{c_1c_2}$. We have that $$"
      ],
      "metadata": {
        "id": "kHP1n6RvdSem"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3BG7QV3j5fK"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 3: A deeper dive into neural network implementations [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hIpMzMHj5fK"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glSwAQZNj5fL",
        "outputId": "dd93a1e4-8966-48f6-adcd-8599937ab930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9912422/9912422 [00:00<00:00, 115456656.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28881/28881 [00:00<00:00, 28576478.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1648877/1648877 [00:00<00:00, 37593721.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 22203413.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [00:04<00:00, 41955632.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download datasets\n",
        "train_set_mnist = torchvision.datasets.MNIST(root=\"./\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set_mnist = torchvision.datasets.MNIST(root=\"./\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)\n",
        "\n",
        "train_set_cifar = torchvision.datasets.CIFAR10(root=\"./\", download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set_cifar = torchvision.datasets.CIFAR10(root=\"./\",download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vswZJ6x7j5fN",
        "outputId": "5ecc18cc-8f5f-4640-f54b-f4c4f7c28a3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e8489066710>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 1843211\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_bBSuCfj5fN"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.1: Implementations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH8W821Fj5fN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNesFurzj5fN"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
        "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code.\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with ReLU activation and softmax output.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input.\n",
        "        nclass (int): The number of classes.\n",
        "        width (int): The width of the hidden layers.\n",
        "        depth (int): The number of hidden layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, nclass, width, depth):\n",
        "        # Call the parent constructor\n",
        "        super().__init__()\n",
        "        # Define the parameters\n",
        "        self.dim = dim\n",
        "        self.nclass = nclass\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        # Define the layers\n",
        "        self.layers = nn.ModuleList([nn.Flatten()])\n",
        "        self.layers.extend([nn.Linear(self.dim, self.width)])\n",
        "        self.layers.extend([nn.ReLU()]) # Add ReLU activation function as every Linear layer is followed by a ReLU activation function\n",
        "        # Define the hidden layers\n",
        "        for i in range(self.depth-1):\n",
        "            self.layers.extend([nn.Linear(self.width, self.width), nn.ReLU()])\n",
        "        # Define the output layer\n",
        "        self.layers.extend([nn.Linear(self.width, self.nclass)])\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass\n",
        "        x = input\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMMofLwHj5fO"
      },
      "outputs": [],
      "source": [
        "def loading_data(batch_size, train_set, test_set):\n",
        "    \"\"\"\n",
        "    This function loads the data using the torch.utils.data.DataLoader function.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): The batch size.\n",
        "        train_set (torch.utils.data.Dataset): The training set.\n",
        "        test_set (torch.utils.data.Dataset): The test set.\n",
        "\n",
        "    Returns:\n",
        "        trainloader (torch.utils.data.DataLoader): The training set loader.\n",
        "        testloader (torch.utils.data.DataLoader): The test set loader.\n",
        "    \"\"\"\n",
        "    # Load the data\n",
        "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBf-6piTj5fO"
      },
      "outputs": [],
      "source": [
        "def train_epoch(trainloader, net, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Trains the network for one epoch.\n",
        "\n",
        "    Args:\n",
        "        trainloader: The training data loader.\n",
        "        net: The network to train.\n",
        "        optimizer: The optimizer to use.\n",
        "        criterion: The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        The average train loss over the epoch.\n",
        "        The train error over the epoch.\n",
        "    \"\"\"\n",
        "    # Set the network to training mode\n",
        "    net.train()\n",
        "    # Initialize the loss and error\n",
        "    total_loss = 0\n",
        "    total_error = 0\n",
        "    # Loop over the training set\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update the loss and error\n",
        "        total_loss += loss.item()\n",
        "        total_error += (outputs.argmax(dim=1) != labels).sum().item()\n",
        "\n",
        "\n",
        "    return total_loss / len(trainloader), total_error / len(trainloader.dataset) # Return the average train loss and train error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZNeA4RQj5fO"
      },
      "outputs": [],
      "source": [
        "def test_epoch(testloader, net, criterion):\n",
        "    \"\"\"\n",
        "    Tests the network for one epoch.\n",
        "\n",
        "    Args:\n",
        "        testloader: The test data loader.\n",
        "        net: The network to test.\n",
        "        criterion: The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        The average test loss over the epoch.\n",
        "        The test error over the epoch.\n",
        "    \"\"\"\n",
        "    # Set the network to evaluation mode\n",
        "    net.eval()\n",
        "    # Initialize the loss and error\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Loop over the test set\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(testloader):\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Update the loss and error\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return test_loss / len(testloader), 1 - correct / total # Return the average test loss and test error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbIOulgoj5fP"
      },
      "outputs": [],
      "source": [
        "def main(batch_size, dim, nclass, width, depth, lr, epochs, train_set, test_set, Adam=True, momentum=None, early_stop_loss=None ,verbose=True):\n",
        "    \"\"\"\n",
        "    This function runs the training and testing epochs.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): The batch size.\n",
        "        dim (int): The dimension of the input.\n",
        "        nclass (int): The number of classes.\n",
        "        width (int): The width of the hidden layers.\n",
        "        depth (int): The number of hidden layers.\n",
        "        lr (float): The learning rate.\n",
        "        epochs (int): The number of epochs.\n",
        "        train_set (torch.utils.data.Dataset): The training set.\n",
        "        test_set (torch.utils.data.Dataset): The test set.\n",
        "        Adam (bool): Whether to use Adam or SGD.\n",
        "        momentum (float): The momentum to use for SGD.\n",
        "        early_stop_loss (float): The loss threshold to stop the training.\n",
        "        verbose (bool): Whether to print the results.\n",
        "\n",
        "    Returns:\n",
        "        The train loss over the epochs.\n",
        "        The test loss over the epochs.\n",
        "        The train error over the epochs.\n",
        "        The test error over the epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # load data\n",
        "    trainloader, testloader = loading_data(batch_size, train_set, test_set)\n",
        "\n",
        "    # define network\n",
        "    net = Net(dim, nclass, width, depth)\n",
        "\n",
        "    # define criterion function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # define Adam or SGD optimizer\n",
        "    if Adam:\n",
        "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    else:\n",
        "        if momentum is None:\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "    # Storing the results for each epoch\n",
        "    store_train_loss = []\n",
        "    store_test_loss = []\n",
        "    store_train_error = []\n",
        "    store_test_error = []\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Training and testing the network\n",
        "        train_loss, train_error = train_epoch(trainloader, net, optimizer, criterion)\n",
        "        test_loss, test_error = test_epoch(testloader, net, criterion)\n",
        "        # Storing the results\n",
        "        store_train_loss.append(train_loss)\n",
        "        store_test_loss.append(test_loss)\n",
        "        store_train_error.append(train_error)\n",
        "        store_test_error.append(test_error)\n",
        "        # Printing the results\n",
        "        if verbose:\n",
        "            print(f\"Epoch: {epoch:03} | Train Loss: {train_loss:.04} | Test Loss: {test_loss:.04} | Train Error: {train_error:.04} | Test Error: {test_error:.04}\")\n",
        "        # Early stopping if the loss is below a certain threshold (i.e. convergence is attained)\n",
        "        if early_stop_loss:\n",
        "            if train_loss < early_stop_loss:\n",
        "                print(f\"Early stopping at epoch {epoch} with train loss {train_loss}\")\n",
        "                break\n",
        "    return store_train_loss, store_test_loss, store_train_error, store_test_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU0pm_38j5fP"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.2: Numerical exploration [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUAIoHa2j5fP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaqbsYxuj5fP",
        "outputId": "8801a4a9-440a-4025-e894-ec9e57012dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depth: 1\n",
            "Epoch: 001 | Train Loss: 0.2574 | Test Loss: 0.1286 | Train Error: 0.07257 | Test Error: 0.0385\n",
            "Epoch: 002 | Train Loss: 0.1068 | Test Loss: 0.1111 | Train Error: 0.03213 | Test Error: 0.0337\n",
            "Epoch: 003 | Train Loss: 0.06968 | Test Loss: 0.08675 | Train Error: 0.02153 | Test Error: 0.0278\n",
            "Epoch: 004 | Train Loss: 0.05071 | Test Loss: 0.06963 | Train Error: 0.01588 | Test Error: 0.0203\n",
            "Epoch: 005 | Train Loss: 0.03749 | Test Loss: 0.06968 | Train Error: 0.01185 | Test Error: 0.0219\n",
            "Epoch: 006 | Train Loss: 0.02917 | Test Loss: 0.07598 | Train Error: 0.0095 | Test Error: 0.0215\n",
            "Epoch: 007 | Train Loss: 0.02152 | Test Loss: 0.07441 | Train Error: 0.007067 | Test Error: 0.0206\n",
            "Epoch: 008 | Train Loss: 0.01773 | Test Loss: 0.08443 | Train Error: 0.0055 | Test Error: 0.024\n",
            "Epoch: 009 | Train Loss: 0.01449 | Test Loss: 0.07609 | Train Error: 0.0044 | Test Error: 0.0221\n",
            "Epoch: 010 | Train Loss: 0.01258 | Test Loss: 0.08071 | Train Error: 0.003933 | Test Error: 0.0201\n",
            "Epoch: 011 | Train Loss: 0.009912 | Test Loss: 0.08216 | Train Error: 0.002983 | Test Error: 0.0222\n",
            "Epoch: 012 | Train Loss: 0.008385 | Test Loss: 0.08764 | Train Error: 0.00275 | Test Error: 0.0209\n",
            "Epoch: 013 | Train Loss: 0.009246 | Test Loss: 0.09054 | Train Error: 0.0031 | Test Error: 0.021\n",
            "Epoch: 014 | Train Loss: 0.007154 | Test Loss: 0.1052 | Train Error: 0.002283 | Test Error: 0.0225\n",
            "Epoch: 015 | Train Loss: 0.007225 | Test Loss: 0.108 | Train Error: 0.002433 | Test Error: 0.0224\n",
            "Epoch: 016 | Train Loss: 0.006315 | Test Loss: 0.09566 | Train Error: 0.002117 | Test Error: 0.0196\n",
            "Epoch: 017 | Train Loss: 0.005191 | Test Loss: 0.1011 | Train Error: 0.0017 | Test Error: 0.0198\n",
            "Epoch: 018 | Train Loss: 0.006066 | Test Loss: 0.1166 | Train Error: 0.001933 | Test Error: 0.0224\n",
            "Epoch: 019 | Train Loss: 0.006028 | Test Loss: 0.1052 | Train Error: 0.001933 | Test Error: 0.0201\n",
            "Epoch: 020 | Train Loss: 0.005225 | Test Loss: 0.1017 | Train Error: 0.00165 | Test Error: 0.0186\n",
            "Depth: 5\n",
            "Epoch: 001 | Train Loss: 0.2769 | Test Loss: 0.139 | Train Error: 0.08433 | Test Error: 0.0413\n",
            "Epoch: 002 | Train Loss: 0.1188 | Test Loss: 0.1169 | Train Error: 0.03378 | Test Error: 0.0334\n",
            "Epoch: 003 | Train Loss: 0.08534 | Test Loss: 0.111 | Train Error: 0.02448 | Test Error: 0.0302\n",
            "Epoch: 004 | Train Loss: 0.06889 | Test Loss: 0.09843 | Train Error: 0.01975 | Test Error: 0.0271\n",
            "Epoch: 005 | Train Loss: 0.05732 | Test Loss: 0.08572 | Train Error: 0.01612 | Test Error: 0.0223\n",
            "Epoch: 006 | Train Loss: 0.05143 | Test Loss: 0.08762 | Train Error: 0.01423 | Test Error: 0.0221\n",
            "Epoch: 007 | Train Loss: 0.04189 | Test Loss: 0.09517 | Train Error: 0.01193 | Test Error: 0.0217\n",
            "Epoch: 008 | Train Loss: 0.03872 | Test Loss: 0.09389 | Train Error: 0.01072 | Test Error: 0.0224\n",
            "Epoch: 009 | Train Loss: 0.03349 | Test Loss: 0.103 | Train Error: 0.009383 | Test Error: 0.023\n",
            "Epoch: 010 | Train Loss: 0.03125 | Test Loss: 0.1103 | Train Error: 0.009033 | Test Error: 0.0205\n",
            "Epoch: 011 | Train Loss: 0.02937 | Test Loss: 0.09502 | Train Error: 0.0082 | Test Error: 0.021\n",
            "Epoch: 012 | Train Loss: 0.02734 | Test Loss: 0.129 | Train Error: 0.006833 | Test Error: 0.0217\n",
            "Epoch: 013 | Train Loss: 0.02559 | Test Loss: 0.08879 | Train Error: 0.007267 | Test Error: 0.0204\n",
            "Epoch: 014 | Train Loss: 0.0193 | Test Loss: 0.1173 | Train Error: 0.00545 | Test Error: 0.0206\n",
            "Epoch: 015 | Train Loss: 0.02484 | Test Loss: 0.09387 | Train Error: 0.00635 | Test Error: 0.0188\n",
            "Epoch: 016 | Train Loss: 0.02362 | Test Loss: 0.1067 | Train Error: 0.006 | Test Error: 0.0206\n",
            "Epoch: 017 | Train Loss: 0.01903 | Test Loss: 0.1217 | Train Error: 0.004917 | Test Error: 0.0179\n",
            "Epoch: 018 | Train Loss: 0.01995 | Test Loss: 0.1381 | Train Error: 0.004583 | Test Error: 0.0167\n",
            "Epoch: 019 | Train Loss: 0.01617 | Test Loss: 0.1367 | Train Error: 0.004267 | Test Error: 0.0178\n",
            "Epoch: 020 | Train Loss: 0.01847 | Test Loss: 0.1085 | Train Error: 0.005317 | Test Error: 0.0184\n",
            "Depth: 10\n",
            "Epoch: 001 | Train Loss: 0.4911 | Test Loss: 0.2384 | Train Error: 0.158 | Test Error: 0.0534\n",
            "Epoch: 002 | Train Loss: 0.2063 | Test Loss: 0.1726 | Train Error: 0.05018 | Test Error: 0.0402\n",
            "Epoch: 003 | Train Loss: 0.1748 | Test Loss: 0.1972 | Train Error: 0.04243 | Test Error: 0.0476\n",
            "Epoch: 004 | Train Loss: 0.1391 | Test Loss: 0.1799 | Train Error: 0.03343 | Test Error: 0.039\n",
            "Epoch: 005 | Train Loss: 0.1323 | Test Loss: 0.1307 | Train Error: 0.03078 | Test Error: 0.0279\n",
            "Epoch: 006 | Train Loss: 0.1086 | Test Loss: 0.1245 | Train Error: 0.02625 | Test Error: 0.0284\n",
            "Epoch: 007 | Train Loss: 0.1049 | Test Loss: 0.1424 | Train Error: 0.0243 | Test Error: 0.0303\n",
            "Epoch: 008 | Train Loss: 0.08606 | Test Loss: 0.1545 | Train Error: 0.02105 | Test Error: 0.0282\n",
            "Epoch: 009 | Train Loss: 0.08042 | Test Loss: 0.1179 | Train Error: 0.01962 | Test Error: 0.0236\n",
            "Epoch: 010 | Train Loss: 0.06896 | Test Loss: 0.138 | Train Error: 0.01658 | Test Error: 0.028\n",
            "Epoch: 011 | Train Loss: 0.07048 | Test Loss: 0.1198 | Train Error: 0.01673 | Test Error: 0.0246\n",
            "Epoch: 012 | Train Loss: 0.05799 | Test Loss: 0.1127 | Train Error: 0.01367 | Test Error: 0.0236\n",
            "Epoch: 013 | Train Loss: 0.05947 | Test Loss: 0.1333 | Train Error: 0.01425 | Test Error: 0.0259\n",
            "Epoch: 014 | Train Loss: 0.0558 | Test Loss: 0.1227 | Train Error: 0.01295 | Test Error: 0.0228\n",
            "Epoch: 015 | Train Loss: 0.04727 | Test Loss: 0.1386 | Train Error: 0.01037 | Test Error: 0.0235\n",
            "Epoch: 016 | Train Loss: 0.0525 | Test Loss: 0.1213 | Train Error: 0.0122 | Test Error: 0.0244\n",
            "Epoch: 017 | Train Loss: 0.04213 | Test Loss: 0.1093 | Train Error: 0.01007 | Test Error: 0.0222\n",
            "Epoch: 018 | Train Loss: 0.04578 | Test Loss: 0.1562 | Train Error: 0.01053 | Test Error: 0.0238\n",
            "Epoch: 019 | Train Loss: 0.03581 | Test Loss: 0.1265 | Train Error: 0.00845 | Test Error: 0.0205\n",
            "Epoch: 020 | Train Loss: 0.04567 | Test Loss: 0.1323 | Train Error: 0.01048 | Test Error: 0.0257\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "depths = [1, 5, 10] # Varying hyperparameter\n",
        "width = 256 # Fixed hyperparameter according to exercise guidelines\n",
        "lr = 0.001\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "dim = 784\n",
        "nclass = 10\n",
        "Adam = True\n",
        "\n",
        "# Store the results\n",
        "store_depth = {}\n",
        "\n",
        "# Run the main function for different depths\n",
        "for depth in depths:\n",
        "    print(f\"Depth: {depth}\")\n",
        "    train_loss, test_loss, train_error, test_error = main(batch_size, dim, nclass, width, depth, lr, epochs, train_set_mnist, test_set_mnist, Adam=Adam)\n",
        "    store_depth[depth] = [train_loss, test_loss, train_error, test_error]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gm-ZYS2j5fQ",
        "outputId": "0b99b034-7fc0-4406-9e0c-3903dae089f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depth: 1\n",
            "Epoch: 001 | Train Loss: 0.2537 | Test Loss: 0.1248 | Train Error: 0.07183 | Test Error: 0.0378\n",
            "Epoch: 002 | Train Loss: 0.1038 | Test Loss: 0.0941 | Train Error: 0.03075 | Test Error: 0.029\n",
            "Epoch: 003 | Train Loss: 0.06865 | Test Loss: 0.0724 | Train Error: 0.02123 | Test Error: 0.0226\n",
            "Epoch: 004 | Train Loss: 0.04997 | Test Loss: 0.0718 | Train Error: 0.01578 | Test Error: 0.023\n",
            "Epoch: 005 | Train Loss: 0.03693 | Test Loss: 0.07329 | Train Error: 0.0115 | Test Error: 0.0215\n",
            "Epoch: 006 | Train Loss: 0.02719 | Test Loss: 0.07352 | Train Error: 0.0087 | Test Error: 0.0225\n",
            "Epoch: 007 | Train Loss: 0.02259 | Test Loss: 0.08354 | Train Error: 0.0069 | Test Error: 0.0222\n",
            "Epoch: 008 | Train Loss: 0.01704 | Test Loss: 0.0758 | Train Error: 0.005367 | Test Error: 0.0217\n",
            "Epoch: 009 | Train Loss: 0.01367 | Test Loss: 0.07454 | Train Error: 0.003867 | Test Error: 0.0193\n",
            "Epoch: 010 | Train Loss: 0.01216 | Test Loss: 0.0752 | Train Error: 0.003867 | Test Error: 0.0199\n",
            "Epoch: 011 | Train Loss: 0.009319 | Test Loss: 0.07253 | Train Error: 0.003017 | Test Error: 0.0203\n",
            "Epoch: 012 | Train Loss: 0.01057 | Test Loss: 0.07927 | Train Error: 0.00355 | Test Error: 0.0196\n",
            "Epoch: 013 | Train Loss: 0.007258 | Test Loss: 0.08663 | Train Error: 0.002217 | Test Error: 0.0187\n",
            "Epoch: 014 | Train Loss: 0.008085 | Test Loss: 0.08164 | Train Error: 0.002867 | Test Error: 0.0183\n",
            "Epoch: 015 | Train Loss: 0.006608 | Test Loss: 0.09155 | Train Error: 0.002383 | Test Error: 0.0206\n",
            "Epoch: 016 | Train Loss: 0.00515 | Test Loss: 0.09755 | Train Error: 0.001533 | Test Error: 0.0192\n",
            "Epoch: 017 | Train Loss: 0.006701 | Test Loss: 0.09191 | Train Error: 0.002417 | Test Error: 0.0182\n",
            "Epoch: 018 | Train Loss: 0.007066 | Test Loss: 0.09125 | Train Error: 0.0023 | Test Error: 0.0186\n",
            "Epoch: 019 | Train Loss: 0.004009 | Test Loss: 0.09586 | Train Error: 0.001367 | Test Error: 0.0192\n",
            "Epoch: 020 | Train Loss: 0.005352 | Test Loss: 0.1066 | Train Error: 0.001817 | Test Error: 0.02\n",
            "Depth: 5\n",
            "Epoch: 001 | Train Loss: 0.2878 | Test Loss: 0.1396 | Train Error: 0.08828 | Test Error: 0.0408\n",
            "Epoch: 002 | Train Loss: 0.1216 | Test Loss: 0.113 | Train Error: 0.03427 | Test Error: 0.0308\n",
            "Epoch: 003 | Train Loss: 0.09056 | Test Loss: 0.1043 | Train Error: 0.02512 | Test Error: 0.028\n",
            "Epoch: 004 | Train Loss: 0.06983 | Test Loss: 0.089 | Train Error: 0.02008 | Test Error: 0.0243\n",
            "Epoch: 005 | Train Loss: 0.05787 | Test Loss: 0.09292 | Train Error: 0.01675 | Test Error: 0.0228\n",
            "Epoch: 006 | Train Loss: 0.04921 | Test Loss: 0.09281 | Train Error: 0.01413 | Test Error: 0.0212\n",
            "Epoch: 007 | Train Loss: 0.0417 | Test Loss: 0.1011 | Train Error: 0.01172 | Test Error: 0.0258\n",
            "Epoch: 008 | Train Loss: 0.0388 | Test Loss: 0.09864 | Train Error: 0.01088 | Test Error: 0.0243\n",
            "Epoch: 009 | Train Loss: 0.03137 | Test Loss: 0.09548 | Train Error: 0.008917 | Test Error: 0.0206\n",
            "Epoch: 010 | Train Loss: 0.02957 | Test Loss: 0.1036 | Train Error: 0.008433 | Test Error: 0.0222\n",
            "Epoch: 011 | Train Loss: 0.02856 | Test Loss: 0.1102 | Train Error: 0.00755 | Test Error: 0.0222\n",
            "Epoch: 012 | Train Loss: 0.02285 | Test Loss: 0.1239 | Train Error: 0.0068 | Test Error: 0.023\n",
            "Epoch: 013 | Train Loss: 0.0261 | Test Loss: 0.1463 | Train Error: 0.006783 | Test Error: 0.0266\n",
            "Epoch: 014 | Train Loss: 0.02099 | Test Loss: 0.1691 | Train Error: 0.006017 | Test Error: 0.0232\n",
            "Epoch: 015 | Train Loss: 0.02137 | Test Loss: 0.1731 | Train Error: 0.006217 | Test Error: 0.0201\n",
            "Epoch: 016 | Train Loss: 0.01856 | Test Loss: 0.1201 | Train Error: 0.005133 | Test Error: 0.022\n",
            "Epoch: 017 | Train Loss: 0.01685 | Test Loss: 0.1223 | Train Error: 0.0045 | Test Error: 0.02\n",
            "Epoch: 018 | Train Loss: 0.02133 | Test Loss: 0.1236 | Train Error: 0.005517 | Test Error: 0.0231\n",
            "Epoch: 019 | Train Loss: 0.01771 | Test Loss: 0.1374 | Train Error: 0.004267 | Test Error: 0.0223\n",
            "Epoch: 020 | Train Loss: 0.01562 | Test Loss: 0.1506 | Train Error: 0.004133 | Test Error: 0.024\n",
            "Depth: 10\n",
            "Epoch: 001 | Train Loss: 0.6258 | Test Loss: 0.1851 | Train Error: 0.221 | Test Error: 0.0472\n",
            "Epoch: 002 | Train Loss: 0.2229 | Test Loss: 0.2023 | Train Error: 0.05575 | Test Error: 0.0446\n",
            "Epoch: 003 | Train Loss: 0.1726 | Test Loss: 0.1448 | Train Error: 0.04282 | Test Error: 0.0346\n",
            "Epoch: 004 | Train Loss: 0.1435 | Test Loss: 0.1326 | Train Error: 0.0347 | Test Error: 0.0319\n",
            "Epoch: 005 | Train Loss: 0.1185 | Test Loss: 0.1412 | Train Error: 0.0286 | Test Error: 0.0366\n",
            "Epoch: 006 | Train Loss: 0.1119 | Test Loss: 0.1498 | Train Error: 0.0266 | Test Error: 0.0331\n",
            "Epoch: 007 | Train Loss: 0.1042 | Test Loss: 0.1409 | Train Error: 0.02487 | Test Error: 0.0356\n",
            "Epoch: 008 | Train Loss: 0.08909 | Test Loss: 0.124 | Train Error: 0.02098 | Test Error: 0.0282\n",
            "Epoch: 009 | Train Loss: 0.08066 | Test Loss: 0.1126 | Train Error: 0.01938 | Test Error: 0.0248\n",
            "Epoch: 010 | Train Loss: 0.07313 | Test Loss: 0.1348 | Train Error: 0.01753 | Test Error: 0.0289\n",
            "Epoch: 011 | Train Loss: 0.073 | Test Loss: 0.1252 | Train Error: 0.01743 | Test Error: 0.0247\n",
            "Epoch: 012 | Train Loss: 0.06787 | Test Loss: 0.1248 | Train Error: 0.01572 | Test Error: 0.0258\n",
            "Epoch: 013 | Train Loss: 0.05634 | Test Loss: 0.1311 | Train Error: 0.0137 | Test Error: 0.0239\n",
            "Epoch: 014 | Train Loss: 0.05915 | Test Loss: 0.1222 | Train Error: 0.01345 | Test Error: 0.0273\n",
            "Epoch: 015 | Train Loss: 0.04851 | Test Loss: 0.1221 | Train Error: 0.01148 | Test Error: 0.0233\n",
            "Epoch: 016 | Train Loss: 0.04977 | Test Loss: 0.1359 | Train Error: 0.0117 | Test Error: 0.0236\n",
            "Epoch: 017 | Train Loss: 0.03868 | Test Loss: 0.1427 | Train Error: 0.0088 | Test Error: 0.0203\n",
            "Epoch: 018 | Train Loss: 0.04389 | Test Loss: 0.1384 | Train Error: 0.0101 | Test Error: 0.0232\n",
            "Epoch: 019 | Train Loss: 0.03554 | Test Loss: 0.1773 | Train Error: 0.008183 | Test Error: 0.0202\n",
            "Epoch: 020 | Train Loss: 0.04175 | Test Loss: 0.1228 | Train Error: 0.008967 | Test Error: 0.0236\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "depths = [1, 5, 10] # Varying hyperparameter\n",
        "width = 256 # Fixed hyperparameter according to exercise guidelines\n",
        "lr = 0.001\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "dim = 784\n",
        "nclass = 10\n",
        "Adam = True\n",
        "\n",
        "# Store the results\n",
        "store_depth = {}\n",
        "\n",
        "# Run the main function for different depths\n",
        "for depth in depths:\n",
        "    print(f\"Depth: {depth}\")\n",
        "    train_loss, test_loss, train_error, test_error = main(batch_size, dim, nclass, width, depth, lr, epochs, train_set_mnist, test_set_mnist, Adam=Adam)\n",
        "    store_depth[depth] = [train_loss, test_loss, train_error, test_error]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3YQgw9wej5fh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dbuw_nBOj5fh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI7QJ8gBj5fh"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HKZqQU2j5fi"
      },
      "source": [
        "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDRR0PWuWj1s"
      },
      "source": [
        "We consider now a fully-connected single hidden-layer neural networ. The input is of dimensionality $N_0$ and we have $N_1$ hidden nodes in layer 1 and $N_2$ output nodes. The activation function is given by $\\phi$. The output for node $i$ in the first layer is given by\n",
        "$$f_i^{(1)}(x)=\\sum_{j=1}^{N_0}w_{ij}^{(1)}x_j+b_i^{(1)},$$\n",
        "where $w_{ij}^{(1)}$ is the component of the weight matrix connecting node $i$ in layer 1 with inpu $j$ and $b_i^{(1)}$ is the bias we add in layer 1 to output node $i$. This is passed though the nonlinearity to obtain\n",
        "$$g_i^{(1)}(x)=\\phi(f_i^{(1)}(x))$$\n",
        "The output layer is consequently given by\n",
        "$$f_i^{(2)}(x)=\\sum_{j=1}^{N_1}w_{ij}^{(2)}g_j^{(1)}(x)+b_i^{(2)}$$\n",
        "\n",
        "We assume an i.i.d distribution on the parameters, namely\n",
        "\n",
        "\\begin{align*}\n",
        "  w_{ij}^{(l)}&\\overset{\\text{i.i.d}}{\\sim} N(0, C_w^{(l)})\\\\\n",
        "  b_{i}^{(l)}&\\overset{\\text{i.i.d}}{\\sim} N(0, \\sigma_b^{(l)})\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diEcMfCrj5fi"
      },
      "source": [
        "### Task 1: Proper weight scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdT4xSfuj5fi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3r5SKQdj5fi"
      },
      "source": [
        "### Task 2: Derive the GP relation for a single hidden layer\n",
        "\n",
        "Note that because weights and biases are taken to be i.i.d, the post activations $g_j^{(1)}, g_{j'}^{(1)}$ are independent for $j\\neq j'$.\n",
        "\n",
        "We aim to show that $f_i^{(2)}(x)$ converges to a Gaussian process with mean $\\mu^1$ and covariance matrix $K^1$. Since $f_i^{(2)}(x)$ is a sum of i.i.d terms, it follows from the Central Limit Theorem that in the limit $N_1\\to\\infty$, $f_i^{(2)}(x)$ will be Gaussian distributed. We read from the definition that the hidden-to-output layer interactions can be written in matrix-vector multiplcations as\n",
        "$$f^{(2)}=w^{(2)}g^{(1)}(x)+b^{(2)}$$Therefore, from the *multivariate* Central Limit Theorem, any finite collection $\\{f_i^{(2)}(x^{\\alpha_1}), \\dots, f_i^{(2)}(x^{\\alpha_k})\\}$ for distinct inputs $\\alpha_1, \\dots ,\\alpha_k$ will have a joint multivariate Gaussian distribution, which is exactly the definition of a Gaussian process. We therefore conclude that $f_i^{(2)}\\sim \\mathcal{GP}(\\mu^1, K^1)$ for some $\\mu^1, K^1$ to be determined.\n",
        "\n",
        "Because parameters are zero mean, we conclude $\\mu^1(x)=0$. To calculate covariance matrix $K$ recall that\n",
        "\\begin{align*}\n",
        "K^1(x,x')&=\\mathbb{E}\\left[f_i^{(2)}(x)f_i^{(2)}(x')\\right]\\\\\n",
        "&= \\sigma_b^2+\\sigma_w^2\\mathbb{E}\\left[g_i^{(1)}(x)g_i^{(1)}(x')\\right]\n",
        "\\end{align*}\n",
        "where the expectation in the RHS depends on the choice of activation function $\\phi$ and whose analytical computation will require integrating over the distribution of $w^{(1)}, b^{(1)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39AftRkTj5fi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zu6ciM9j5fj"
      },
      "source": [
        "### Task 3: Why in succession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Os8Mz4j5fj"
      },
      "source": [
        "We seek to extend this argument to deeper layers inductively; therefore, we require using the argument above that the previous layer is already distributed according to a Gaussian process. As such, we take the limits $N_1\\to\\infty, N_2\\to\\infty\\dots$ in succession to allow application of the induction hypothesis.\n",
        "\n",
        "We should note it is possible to derive this GP relation without sequential limits; see for example Appendix C of [Lee et al. (2018)](https://link.springer.com/chapter/10.1007/978-1-4612-0745-0_2).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ4EV355j5fj"
      },
      "source": [
        "### Task 4: Derive the GP relation for multiple hidden layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mzu4zSTj5fj"
      },
      "source": [
        "We will show that for arbitrary layer $l$ the output\n",
        "\\begin{equation}f_i^{(l)}(x)=\\sum_{j=1}^{N_l}w_{ij}^{(l)}g_j^{(l-1)}(x)+b_i^{(l)}\\end{equation}\n",
        "with $g_J^{(l-1)}(x)=\\phi(f_j^{(l-1)}(x))$\n",
        "is a Gaussian process with mean $\\mu^l$ and covariance matrix $K^l$.\n",
        "\n",
        "We assume that $f_i^{(l-1)}$ is a Gaussian process, i.i.d for each $i$, and that $g_i^{(l-1)}(x)$ are i.i.d. The computation in the $l$th layer is as as above. As before, $f_i^{(l)}(x)$ is a sum of i.i.d random variables so that as $N_l\\to\\infty$ any finite collection $\\{f_i^{(l)}(x^{\\alpha_1}), \\dots, f_i^{(l)}(x^{\\alpha_k})\\}$ will have a joint multivariate Gaussian distribution by the multivariate Central Limit Theorem. We can apply this because of the scaling of the variance $C_w^{(l)}=\\frac{\\sigma_w^{(l)}}{N_{l-1}}$ which ensures that in each layer the variance is finite and constant.\n",
        "\n",
        "The variables $\\{f_i^{(l)}$ will therefore be disributed according to a GP with mean $\\mu^l$ and covariance $K^l$. As before, the mean $\\mu^l$ is trivially zero. The covariance is given by\n",
        "\\begin{align*}\n",
        "K^l(x,x')&=\\mathbb{E}\\left[\\left(f_i^{(l)}(x)-\\mu^l\\right)\\left(f_i^{(l)}(x')-\\mu^l\\right)\\right]\\\\\n",
        "&= \\sigma_b^2+\\sigma_w^2\\mathbb{E}\\left[g_i^{(l-1)}(x)g_i^{(l-1)}(x')\\right]\\\\\n",
        "&=\\sigma_b^2+\\sigma_w^2\\mathbb{E}\\left[\\phi\\left(f_i^{(l-1)}(x)\\right)\\phi\\left(f_i^{(l-1)}(x')\\right)\\right]\n",
        "\\end{align*}\n",
        "\n",
        "where the expectation is taken over the Gaussian process $f_i^{(l-1)}\\sim \\mathcal{GP}(0, K^{l-1})$.\n",
        "\n",
        "-make ot clear why vali to apply CLT\n",
        "-derive recursive expression for $K^l$ whih iwill contain expectation over previous layer's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3aKJJ0jj5fj"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFOiXMyUj5fj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rhqWM74j5fj",
        "outputId": "d7e094ee-7878-4b52-da40-0fc678dc8af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [00:02<00:00, 59666048.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training X shape and type: torch.Size([1000, 3072]) torch.float64\n",
            "Training y shape and type: torch.Size([1000]) torch.float64\n",
            "Test X* shape and type: torch.Size([1000, 3072]) torch.float64\n",
            "Test y* shape and type: torch.Size([1000]) torch.float64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# Transformation definition\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Dataset loading\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Class filtering and subsampling\n",
        "class_indices = {1: -0.5, 4: 0.5}\n",
        "filtered_train_indices = [i for i, label in enumerate(trainset.targets) if label in class_indices]\n",
        "filtered_test_indices = [i for i, label in enumerate(testset.targets) if label in class_indices]\n",
        "\n",
        "np.random.seed(42)\n",
        "subsample_train_indices = np.random.choice(filtered_train_indices, 1000, replace=False)\n",
        "np.random.seed(43)\n",
        "subsample_test_indices = np.random.choice(filtered_test_indices, 1000, replace=False)\n",
        "\n",
        "# Prepare datasets\n",
        "subsampled_train_dataset = torch.utils.data.Subset(trainset, subsample_train_indices)\n",
        "subsampled_test_dataset = torch.utils.data.Subset(testset, subsample_test_indices)\n",
        "\n",
        "# Function to extract data as matrices\n",
        "def extract_data(loader):\n",
        "    X, y = [], []\n",
        "    for data in loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.reshape(inputs.shape[0], -1)  # Flatten the image\n",
        "        label = torch.tensor([class_indices[labels.item()]], dtype=torch.float64)  # Remap labels to -0.5 and +0.5\n",
        "        X.append(inputs)\n",
        "        y.append(label)\n",
        "    X = torch.cat(X).to(dtype=torch.float64)  # Convert to float64\n",
        "    y = torch.cat(y).to(dtype=torch.float64)  # Convert to float64\n",
        "    return X, y\n",
        "\n",
        "# Creating DataLoaders\n",
        "trainloader = torch.utils.data.DataLoader(subsampled_train_dataset, batch_size=1, shuffle=False)\n",
        "testloader = torch.utils.data.DataLoader(subsampled_test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Extract matrices\n",
        "X, y = extract_data(trainloader)\n",
        "X_star, y_star = extract_data(testloader)\n",
        "\n",
        "# Print shapes and types to verify\n",
        "print(\"Training X shape and type:\", X.shape, X.dtype)\n",
        "print(\"Training y shape and type:\", y.shape, y.dtype)\n",
        "print(\"Test X* shape and type:\", X_star.shape, X_star.dtype)\n",
        "print(\"Test y* shape and type:\", y_star.shape, y_star.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IkW_cytj5fk"
      },
      "outputs": [],
      "source": [
        "class Kernel:\n",
        "  \"\"\"Base Kernel class. Methods should be overwritten.\"\"\"\n",
        "\n",
        "  def __init__(self, params : dict):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x1, x2):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "class ReLUKernel(Kernel):\n",
        "    def __init__(self, params: dict):\n",
        "        self.L = params['L']\n",
        "        self.sigma_w_sq = params['sigma_w_sq']\n",
        "        self.sigma_b_sq = params['sigma_b_sq']\n",
        "\n",
        "    def __call__(self, X1, X2):\n",
        "      L = self.L\n",
        "      sigma_w = self.sigma_w_sq\n",
        "      sigma_b = self.sigma_b_sq\n",
        "\n",
        "\n",
        "      M1, N = X1.shape\n",
        "      M2, _ = X2.shape\n",
        "\n",
        "      Kxxprime = sigma_b + sigma_w * (X1 @ X2.t() / N)\n",
        "      Kxx_vect = torch.diag(sigma_b + sigma_w * (X1 @ X1.t() / N))\n",
        "      Kxx = Kxx_vect.view(-1, 1).expand(-1, M2)\n",
        "      Kxprimexprime_vect = torch.diag(sigma_b + sigma_w * (X2 @ X2.t() / N))\n",
        "      Kxprimexprime = Kxprimexprime_vect.view(1, -1).expand(M1, -1)\n",
        "\n",
        "      for l in range(1, L + 1):\n",
        "          thetaxxprime = torch.acos(Kxxprime / torch.sqrt(Kxx * Kxprimexprime))\n",
        "          thetaxx = torch.acos(Kxx / torch.sqrt(Kxx * Kxx))\n",
        "          thetaxprimexprime = torch.acos(Kxprimexprime / torch.sqrt(Kxprimexprime * Kxprimexprime))\n",
        "\n",
        "          Kxxprime = sigma_b + (sigma_w / (2 * torch.pi)) * torch.sqrt(Kxx * Kxprimexprime) * (torch.sin(thetaxxprime) + (torch.pi - thetaxxprime) * torch.cos(thetaxxprime))\n",
        "          Kxx = sigma_b + (sigma_w / (2 * torch.pi)) * torch.sqrt(Kxx * Kxx) * (torch.sin(thetaxx) + (torch.pi - thetaxx) * torch.cos(thetaxx))\n",
        "          Kxprimexprime = sigma_b + (sigma_w / (2 * torch.pi)) * torch.sqrt(Kxprimexprime * Kxprimexprime) * (torch.sin(thetaxprimexprime) + (torch.pi - thetaxprimexprime) * torch.cos(thetaxprimexprime))\n",
        "\n",
        "      return Kxxprime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWzlsD8s4q4s"
      },
      "outputs": [],
      "source": [
        "class GP:\n",
        "    def __init__(self, kernel):\n",
        "        \"\"\"\n",
        "        Initialize the Gaussian Process (GP) with a specified kernel.\n",
        "\n",
        "        Args:\n",
        "        kernel (callable): The kernel function to use in the GP.\n",
        "        \"\"\"\n",
        "        self.k = kernel\n",
        "\n",
        "    def predict(self, x_star, X=None, y=None, size=1, sigma=0.):\n",
        "        \"\"\"\n",
        "        Given observations (X, y) and test points x_star, fit a GP model\n",
        "        and draw posterior samples for f(x_star) from the fitted model.\n",
        "\n",
        "        Args:\n",
        "        x_star (torch.Tensor): Test points at which predictions will be made.\n",
        "        X (torch.Tensor): Observed features.\n",
        "        y (torch.Tensor): Observed response variables.\n",
        "        size (int): Number of posterior samples to draw.\n",
        "        sigma (float): Noise level in observations.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: Posterior samples for f(x_star).\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute kernel matrices\n",
        "        k_xs_x   = self.k(x_star, X)       # m x n\n",
        "        k_x_xs   = k_xs_x.T                 # n x m\n",
        "        k_xs_xs  = self.k(x_star, x_star)  # m x m\n",
        "        k_x_x    = self.k(X, X)            # n x n\n",
        "        cov_x_x  = k_x_x + sigma**2 * torch.eye(X.shape[0])  # n x n\n",
        "\n",
        "        # Compute posterior mean and covariance\n",
        "        posterior_mean = torch.matmul(k_xs_x, torch.linalg.solve(cov_x_x, y))\n",
        "        posterior_var = k_xs_xs - torch.matmul(k_xs_x, torch.linalg.solve(cov_x_x, k_x_xs))\n",
        "\n",
        "        # -----------------------------------------------------------------------------------\n",
        "        # Enforce symmetry and positive definiteness that may be lost due to numerical errors\n",
        "        posterior_var = (posterior_var + posterior_var.T) / 2  # Enforce symmetry\n",
        "        # Add a small amount of noise to the diagonal to make the covariance matrix positive definite\n",
        "        posterior_var = posterior_var + 1e-6 * torch.eye(posterior_var.shape[0])\n",
        "        # -----------------------------------------------------------------------------------\n",
        "\n",
        "        self.posterior_mean = posterior_mean\n",
        "        self.posterior_var = posterior_var\n",
        "\n",
        "        # Draw samples from the posterior distribution\n",
        "        y_star = torch.distributions.MultivariateNormal(posterior_mean, posterior_var).sample((size,))\n",
        "\n",
        "        return y_star\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l92lqGfs69Kn"
      },
      "outputs": [],
      "source": [
        "params = {'L': 10, 'sigma_b_sq': 1, 'sigma_w_sq': 1}\n",
        "relu = ReLUKernel(params)\n",
        "gp = GP(relu)\n",
        "\n",
        "y_pred = gp.predict(X_star, X, y, size=1, sigma=0.1)\n",
        "y_pred = torch.where(y_pred > 0, 0.5, -0.5)[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(y_pred == y_star)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BOXIyK5477f",
        "outputId": "d616cb0c-8f9c-4b31-abe3-da8ace2f0f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(817)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lrange = [1, 2, 5, 10]\n",
        "sigma_b_range = [0.01, 0.1, 0.5, 1]\n",
        "sigma_w_range = [0.01, 0.1, 0.5, 1]\n",
        "\n",
        "all_error = np.zeros((len(Lrange), len(sigma_b_range), len(sigma_w_range)))\n",
        "\n",
        "old_error = 1\n",
        "\n",
        "for i, L in enumerate(Lrange):\n",
        "  print(i)\n",
        "  for j, b in enumerate(sigma_b_range):\n",
        "    for k, w in enumerate(sigma_w_range):\n",
        "      params = {'L': L, 'sigma_b_sq': b, 'sigma_w_sq': w}\n",
        "      relu = ReLUKernel(params)\n",
        "      gp = GP(relu)\n",
        "      try:\n",
        "        y_pred = gp.predict(X_star, X, y, size=1, sigma=0.1)\n",
        "        y_pred = torch.where(y_pred > 0, 0.5, -0.5)[0]\n",
        "        new_error = torch.sum(y_pred == y_star) / len(y_pred)\n",
        "      except:\n",
        "        new_error = 0\n",
        "      all_error[i, j, k] = new_error\n",
        "      if new_error > old_error:\n",
        "        best_params = (L, b, w)\n",
        "        old_error = new_error\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifTH4BBKBlB5",
        "outputId": "e22e6b45-3c69-4500-d2b1-ba3bb4f8ca26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(all_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p653C84yJ-Dj",
        "outputId": "6f5c65d8-d9b2-4e44-9e78-2bfa1ddbe0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.911"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1UwxiarO_59",
        "outputId": "c885dc7f-c54f-430a-d4f6-b9cad7f8b2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.673, 0.857, 0.902, 0.882],\n",
              "        [0.678, 0.853, 0.891, 0.877],\n",
              "        [0.705, 0.84 , 0.88 , 0.873],\n",
              "        [0.686, 0.838, 0.875, 0.869]],\n",
              "\n",
              "       [[0.494, 0.786, 0.899, 0.893],\n",
              "        [0.494, 0.763, 0.895, 0.892],\n",
              "        [0.494, 0.764, 0.884, 0.911],\n",
              "        [0.494, 0.759, 0.879, 0.898]],\n",
              "\n",
              "       [[0.494, 0.494, 0.788, 0.88 ],\n",
              "        [0.494, 0.494, 0.797, 0.886],\n",
              "        [0.494, 0.494, 0.788, 0.887],\n",
              "        [0.494, 0.494, 0.793, 0.89 ]],\n",
              "\n",
              "       [[0.494, 0.494, 0.494, 0.787],\n",
              "        [0.   , 0.494, 0.494, 0.796],\n",
              "        [0.   , 0.494, 0.494, 0.821],\n",
              "        [0.   , 0.494, 0.494, 0.8  ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'L': 10, 'sigma_b_sq': 5, 'sigma_w_sq': 1.5}\n",
        "relu = ReLUKernel(params)\n",
        "gp = GP(relu)\n",
        "\n",
        "y_pred = gp.predict(X_star, X, y, size=1, sigma=0.1)\n",
        "y_pred = torch.where(y_pred > 0, 0.5, -0.5)[0]\n",
        "\n",
        "torch.sum(y_pred == y_star)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAKDC2eTSORD",
        "outputId": "c09d0cbb-eba5-40b3-c6d9-6e64cd212f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(890)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.figure()\n",
        "for l in range(4):\n",
        "  plt.plot(np.mean(all_error[l], axis=0), label=f\"l={l}\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "MS_6M6kqK6MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_aR7J8EiLJHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}