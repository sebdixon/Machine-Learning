{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CID: 01843211\n",
    "\n",
    "**Colab link:** insert colab link here\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Quickfire questions [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (True risk / Empirical risk):\n",
    "\n",
    "Let $(\\mathbf{x},\\mathbf{y})$ be sampled from the data-generating distribution $D$. Let $f$ be a classifier in function space $\\hat{\\mathcal{F}}$ and let $L$ be a loss function which is a metric or pseudo-metric. Then the risk $R$ corresponding to $f$ is defined $$R(f)=\\mathbb{E}_D[L(f(\\mathbf{x}, \\mathbf{y})]$$\n",
    "\n",
    "\n",
    "This definition assumes that $(\\mathbf{x}, \\mathbf{y})$ are drawn from the true underlying distribution $\\mathcal{D}$, making $R(f)$ a measure of how well $f$ is expected to perform on the entire data space, not just the observed samples.\n",
    "\n",
    "In statistical learning we seek an $\\hat{f}\\in \\hat{\\mathcal{F}}$ which minimises this true risk, i.e\n",
    "$$\\hat{f}\\in\\text{arg min}_{f\\in\\hat{\\mathcal{F}}}R(f)$$\n",
    "\n",
    "However, since $\\mathcal{D}$ is unknown in real-world scenarios, we cannot compute $R(f)$ directly. Instead, we approximate it using the empirical risk $\\hat{R}(f)$, which is calculated based on the observed dataset $D$:\n",
    "\n",
    "\n",
    "$$\\hat{R}(f)=\\frac{1}{n}\\sum_{i=1}^n L(f(\\mathbf{x}^i), \\mathbb{y}^i)$$\n",
    "\n",
    "For large datasets, this is approximately equal to the true risk and allows us to learn a function\n",
    "\n",
    "$$f^*=\\text{arg min}_{f\\in\\mathcal{F}}\\hat{R}(f)$$\n",
    "\n",
    "where $\\mathcal{F}$ is a choice of function class that we believe is likely to contain the true function, or a suitable approximation thereof. If $\\mathcal{F}$ is well-chosen this will approximate the $\\hat{f}$ defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
    "\n",
    "A rich hypothesis class $\\mathcal{F}$ will result in the learned function minimising the training error. For example, consider a dataset of $n$ points - if the function class is all degree $n-1$ polynomials, then the learned function will perfectly interpolate all points in the dataset. \n",
    "\n",
    "However, this can be undesirable, as often variations in the data are due to random noise. We instead seek a simpler function will will *generalise* better, and thus minimise the *generalisation error*, which is exactly the difference between $f^*$ and $\\hat{f}$ above.\n",
    "\n",
    "Linear regression is an example of a basic function class. We approximate relationships with linear maps from the feature space to the output space. This performs well with simple relationships, and exhibits low variance, however can be biased with more complex, e.g. polynomial, relationships.\n",
    "\n",
    "On th eother hand, neural nets are examples of a very rich function classse can capture much more complex relationships, however will overfit highly, with high variance, for a simple linear map, and therefore will exhibit high generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (Dataset splitting):\n",
    "\n",
    "In the case that validation data is drawn from the same distribution as training data, it would be fair to assume that model performance on unseen data is similar to that on that validation data.\n",
    "\n",
    "However, there are exceptions to this rule. Consider a time series generated by a non-stationary model, for example S&P 500 close. If the training data is 2023 data, and the validation data is Jan/Feb 2024 data, it would be unreasonable to asssume that accuracy will perform as well when evluated on the remainder of 2024 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (Occamâ€™s razor):\n",
    "\n",
    "---\n",
    "\n",
    "Occam's Razor suggests that among competing hypotheses that predict equally well, the simplest one should be selected. This simplicity is often equated with a model or explanation that makes the fewest assumptions.\n",
    "\n",
    "In machine learning, Occam's Razor is interpreted as a guideline for model selection. The principle advises choosing the simplest model that adequately fits the data. This approach is grounded in the idea that simpler models are less likely to overfit the training data. Overfitting occurs when a model captures noise or random fluctuations in the training set rather than the underlying distribution, leading to poor generalisation to new, unseen data.\n",
    "\n",
    "When dealing with naturally occurring data, such as images, the application of Occam's Razor becomes particularly relevant. Images are high-dimensional data with complex, intricate structures. Models trained on image data, such as convolutional neural networks (CNNs), can easily become overly complex, with millions of parameters capable of fitting the training data very closely.\n",
    "\n",
    "Applying Occam's Razor in this context means preferring simpler models that still capture the essential patterns in the image data without memorizing specific details. This simplicity helps in generalizing better to unseen images by focusing on broader, more universal features rather than specifics of the training set.\n",
    "\n",
    "For image data, a simpler model according to Occam's Razor would still need to be complex enough to handle the data's inherent complexity but not so complex that it learns the noise or irrelevant details. This balance helps in achieving good performance on new images that were not part of the training process.\n",
    "\n",
    "Simpler models are often more interpretable and computationally efficient. This efficiency is crucial for deploying models in real-world applications where computational resources may be limited, and understanding model predictions is important for trust and transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 (Generalisation error):\n",
    "\n",
    "\n",
    "The generalisation error of a model quantifies how well the model performs on new, unseen data compared to the training data on which it was trained. For a \"good\" model, the generalisation error should be small. A small generalisation error indicates that the model effectively captures the underlying patterns or distributions of the data without being overly fitted to the noise or specific details of the training set.\n",
    "\n",
    "A model with a small generalisation error has successfully learned the true underlying patterns in the data. This ability suggests that the model can apply what it has learned from the training data to unseen data, making accurate predictions across a variety of scenarios that were not specifically presented during training.\n",
    "\n",
    "A good model strikes an optimal balance between bias (the error from erroneous assumptions in the learning algorithm) and variance (the error from sensitivity to small fluctuations in the training set). A small generalisation error indicates that the model has achieved this balance, being neither too simple (high bias and unable to capture complex patterns) nor too complex (high variance and overfitting to the training data).\n",
    "\n",
    "Overfitting occurs when a model learns the noise or random fluctuations in the training data rather than the actual signal. A model with a small generalisation error is robust against overfitting, meaning it has generalized well from the training data to unseen data, focusing on the signal rather than the noise.\n",
    "\n",
    "The ultimate goal of a machine learning model is to perform well on real-world data, which is often different in various ways from the data used during training. A small generalisation error implies that the model is likely to be effective and reliable when deployed in real-world applications, accurately handling new examples that reflect the complexities and variations of real-world scenarios.\n",
    "\n",
    "A good model, by definition, is one that generalizes well from the training data to unseen data, evidenced by a small generalisation error. Achieving a small generalization error requires careful model design, including selecting the right model complexity, employing proper training techniques, and using strategies such as cross-validation and regularization to prevent overfitting. Ultimately, the small generalization error of a good model reflects its ability to make accurate predictions across a wide range of data, embodying the core goal of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6 (Rademacher complexity pt1):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 (Rademacher complexity pt2):\n",
    "\n",
    "The empirical Rademacher complexity of a function class $ \\mathcal{F} $ with respect to a sample $ S = \\{x_1, x_2, \\ldots, x_n\\} $ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\sigma}\\left[\\sup_{f \\in \\mathcal{F}}\\left(\\frac{2}{n} \\sum_{i=1}^n \\sigma_i f(x_i)\\right)\\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $S$ represents a sample of $n$ points drawn from a distribution $\\mathcal{D}$.\n",
    "- $\\mathcal{F}$ is a class of functions where each function $f$ maps an input space $\\mathcal{X}$ to real numbers $\\mathbb{R}$.\n",
    "- $\\sigma = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)$ is a sequence of independent Rademacher variables, each taking values $-1$ or $+1$ with equal probability.\n",
    "- $\\mathbb{E}_{\\sigma}$ denotes the expectation over all possible realizations of the Rademacher sequence $\\sigma$.\n",
    "\n",
    "The empirical Rademacher complexity measures the expected maximum correlation between the functions in $\\mathcal{F}$ and a random pattern of signs provided by $\\sigma$ on the sample $S$. It serves as an indicator of the function class's ability to fit random noise, which is directly related to its potential for overfitting and its generalization capacity.\n",
    "\n",
    "A high value of empirical Rademacher complexity suggests that the function class $ \\mathcal{F} $ has a strong ability to fit random patterns. Specifically, it means that there exist functions within $ \\mathcal{F} $ that can align closely with random assignments of labels (as represented by the Rademacher variables) to the data points in $ S $. This ability is indicative of $ \\mathcal{F} $'s flexibility or complexity in modeling data. While the capacity to fit data closely might seem desirable, a high empirical Rademacher complexity raises concerns about overfitting. Overfitting occurs when a model captures not just the underlying signal in the data but also the noise. Therefore, a high complexity value warns that models from $ \\mathcal{F} $ might not generalise well to unseen data, as they could be fitting the noise present in the training sample.\n",
    "\n",
    "Mathematically a high empirical Rademacher complexity will result in a looser bound on the generalisation error. Recall:\n",
    "$$\n",
    "\\mathbb{P}_{S \\sim \\mathcal{D}^n}\\left( \\forall f \\in \\mathcal{F},\\, \\left| \\mathbb{E}[L(f, S)] - \\hat{\\mathbb{E}}[L(f, S)] \\right| \\leq 2\\hat{\\mathcal{R}}_S(\\mathcal{F}) + 3\\sqrt{\\frac{\\log(\\frac{2}{\\delta})}{2n}} \\right) \\geq 1 - \\delta\n",
    "$$\n",
    "\n",
    "\n",
    "This bound implies that with high probability (at least $1 - \\delta$), the absolute difference between the true expected loss and the empirical loss for all functions in the class $\\mathcal{F}$ is bounded by the term involving the empirical Rademacher complexity and a term that decreases as the sample size $n$ increases. The presence of $\\hat{\\mathcal{R}}_S(\\mathcal{F})$ in the bound highlights the role of the function class's complexity in determining generalisation performance. A higher empirical Rademacher complexity leads to a looser bound, meaning that models with higher complexity may have a larger difference between their training and test performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8 (Regularisation term in the loss function):\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (Momentum gradient descent):\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10 (Adam):\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11 (AdaGrad):\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12 (Decaying Learning Rate):\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Part 2: Short-ish proofs [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2.1: Bounds on the risk [1 point]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.2: On semi-definiteness [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.3: A quick recap of momentum [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.4: Convergence proof [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 3: A deeper dive into neural network implementations [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [04:54<00:00, 578801.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download datasets\n",
    "train_set_mnist = torchvision.datasets.MNIST(root=\"./\", download=True,\n",
    "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_mnist = torchvision.datasets.MNIST(root=\"./\",download=True,\n",
    "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)\n",
    "\n",
    "train_set_cifar = torchvision.datasets.CIFAR10(root=\"./\", download=True,\n",
    "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_cifar = torchvision.datasets.CIFAR10(root=\"./\",download=True,\n",
    "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1843211\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 3.1: Implementations [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
    "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code.\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with ReLU activation and softmax output.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): The dimension of the input.\n",
    "        nclass (int): The number of classes.\n",
    "        width (int): The width of the hidden layers.\n",
    "        depth (int): The number of hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, nclass, width, depth):\n",
    "        # Call the parent constructor\n",
    "        super().__init__()\n",
    "        # Define the parameters\n",
    "        self.dim = dim\n",
    "        self.nclass = nclass\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        # Define the layers\n",
    "        self.layers = nn.ModuleList([nn.Flatten()])\n",
    "        self.layers.extend([nn.Linear(self.dim, self.width)])\n",
    "        self.layers.extend([nn.ReLU()]) # Add ReLU activation function as every Linear layer is followed by a ReLU activation function\n",
    "        # Define the hidden layers\n",
    "        for i in range(self.depth-1):\n",
    "            self.layers.extend([nn.Linear(self.width, self.width), nn.ReLU()])\n",
    "        # Define the output layer\n",
    "        self.layers.extend([nn.Linear(self.width, self.nclass)])\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Forward pass\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(batch_size, train_set, test_set):\n",
    "    \"\"\"\n",
    "    This function loads the data using the torch.utils.data.DataLoader function.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): The batch size.\n",
    "        train_set (torch.utils.data.Dataset): The training set.\n",
    "        test_set (torch.utils.data.Dataset): The test set.\n",
    "        \n",
    "    Returns:\n",
    "        trainloader (torch.utils.data.DataLoader): The training set loader.\n",
    "        testloader (torch.utils.data.DataLoader): The test set loader.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(trainloader, net, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the network for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        trainloader: The training data loader.\n",
    "        net: The network to train.\n",
    "        optimizer: The optimizer to use.\n",
    "        criterion: The loss function to use.\n",
    "        \n",
    "    Returns:\n",
    "        The average train loss over the epoch.\n",
    "        The train error over the epoch.\n",
    "    \"\"\"\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    # Initialize the loss and error\n",
    "    total_loss = 0\n",
    "    total_error = 0\n",
    "    # Loop over the training set\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update the loss and error\n",
    "        total_loss += loss.item()\n",
    "        total_error += (outputs.argmax(dim=1) != labels).sum().item()\n",
    "\n",
    "    \n",
    "    return total_loss / len(trainloader), total_error / len(trainloader.dataset) # Return the average train loss and train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(testloader, net, criterion):\n",
    "    \"\"\"\n",
    "    Tests the network for one epoch.\n",
    "\n",
    "    Args:\n",
    "        testloader: The test data loader.\n",
    "        net: The network to test.\n",
    "        criterion: The loss function to use.\n",
    "    \n",
    "    Returns:\n",
    "        The average test loss over the epoch.\n",
    "        The test error over the epoch.\n",
    "    \"\"\"\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "    # Initialize the loss and error\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Loop over the test set\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(testloader):\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Update the loss and error\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return test_loss / len(testloader), 1 - correct / total # Return the average test loss and test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(batch_size, dim, nclass, width, depth, lr, epochs, train_set, test_set, Adam=True, momentum=None, early_stop_loss=None ,verbose=True):\n",
    "    \"\"\"\n",
    "    This function runs the training and testing epochs.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): The batch size.\n",
    "        dim (int): The dimension of the input.\n",
    "        nclass (int): The number of classes.\n",
    "        width (int): The width of the hidden layers.\n",
    "        depth (int): The number of hidden layers.\n",
    "        lr (float): The learning rate.\n",
    "        epochs (int): The number of epochs.\n",
    "        train_set (torch.utils.data.Dataset): The training set.\n",
    "        test_set (torch.utils.data.Dataset): The test set.\n",
    "        Adam (bool): Whether to use Adam or SGD.\n",
    "        momentum (float): The momentum to use for SGD.\n",
    "        early_stop_loss (float): The loss threshold to stop the training.\n",
    "        verbose (bool): Whether to print the results.\n",
    "    \n",
    "    Returns:\n",
    "        The train loss over the epochs.\n",
    "        The test loss over the epochs.\n",
    "        The train error over the epochs.\n",
    "        The test error over the epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # load data\n",
    "    trainloader, testloader = loading_data(batch_size, train_set, test_set)\n",
    "\n",
    "    # define network\n",
    "    net = Net(dim, nclass, width, depth)\n",
    "\n",
    "    # define criterion function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # define Adam or SGD optimizer\n",
    "    if Adam:\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        if momentum is None:\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "        else:\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # Storing the results for each epoch\n",
    "    store_train_loss = []\n",
    "    store_test_loss = []\n",
    "    store_train_error = []\n",
    "    store_test_error = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training and testing the network\n",
    "        train_loss, train_error = train_epoch(trainloader, net, optimizer, criterion)\n",
    "        test_loss, test_error = test_epoch(testloader, net, criterion)\n",
    "        # Storing the results\n",
    "        store_train_loss.append(train_loss)\n",
    "        store_test_loss.append(test_loss)\n",
    "        store_train_error.append(train_error)\n",
    "        store_test_error.append(test_error)\n",
    "        # Printing the results\n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch:03} | Train Loss: {train_loss:.04} | Test Loss: {test_loss:.04} | Train Error: {train_error:.04} | Test Error: {test_error:.04}\")\n",
    "        # Early stopping if the loss is below a certain threshold (i.e. convergence is attained)\n",
    "        if early_stop_loss:\n",
    "            if train_loss < early_stop_loss:\n",
    "                print(f\"Early stopping at epoch {epoch} with train loss {train_loss}\")\n",
    "                break\n",
    "    return store_train_loss, store_test_loss, store_train_error, store_test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 3.2: Numerical exploration [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "depths = [1, 5, 10] # Varying hyperparameter\n",
    "width = 256 # Fixed hyperparameter according to exercise guidelines\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "dim = 784\n",
    "nclass = 10\n",
    "Adam = True\n",
    "\n",
    "# Store the results\n",
    "store_depth = {}\n",
    "\n",
    "# Run the main function for different depths \n",
    "for depth in depths:\n",
    "    print(f\"Depth: {depth}\")\n",
    "    train_loss, test_loss, train_error, test_error = main(batch_size, dim, nclass, width, depth, lr, epochs, train_set_cifar, test_set_cifar, Adam=Adam)\n",
    "    store_depth[depth] = [train_loss, test_loss, train_error, test_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Proper weight scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Derive the GP relation for a single hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Why in succession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Derive the GP relation for multiple hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can of course add more cells of both code and markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
